{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dearthdiggler/docs/blob/main/recreate_this_application_with_a_supabase_db_back_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a significant architectural shift from the standalone Python/Streamlit script described in the text to a modern, decoupled web application.\n",
        "\n",
        "To achieve the goal of an \"always available\" and \"super fast\" feel, we must move away from the current model where data processing happens *when the user loads the page*.\n",
        "\n",
        "Instead, we will adopt an asynchronous architecture:\n",
        "\n",
        "1.  **The Backend \"Brain\" (Python Worker):** A background service running on Fly.io. It runs periodically (e.g., nightly or hourly). It uses Firecrawl, OCR, and other tools to crunch the data, calculate scores, and find leads. It then pushes this \"ready-to-serve\" data into Supabase.\n",
        "2.  **The Database (Supabase):** PostgreSQL hosting the cleaned, scored data. It provides an instant API for the frontend.\n",
        "3.  **The Frontend (Vite/React):** A lightweight web application. When it loads, it doesn't do any math. It just asks Supabase for the pre-calculated JSON data, rendering almost instantly.\n",
        "\n",
        "Here is the blueprint and essential code blocks for this new architecture.\n",
        "\n",
        "### The Architecture Diagram\n",
        "\n",
        "```mermaid\n",
        "graph TD\n",
        "    subgraph \"Fly.io (Backend)\"\n",
        "        Scheduler[Cron/Scheduler] --> Worker[Python Atlas Worker Container]\n",
        "        Worker --> Firecrawl[Firecrawl API]\n",
        "        Worker --> OCR[Tesseract OCR]\n",
        "        Worker --> Sources[Reddit/Zillow APIs]\n",
        "        Worker -- \"Upserts clean, scored JSON data\" --> SupabaseDB\n",
        "    end\n",
        "\n",
        "    subgraph \"Supabase (BaaS)\"\n",
        "        SupabaseDB[(PostgreSQL DB)]\n",
        "        Auth[Supabase Auth]\n",
        "        API[Auto-generated REST/GraphQL API]\n",
        "    end\n",
        "\n",
        "    subgraph \"Fly.io (Frontend)\"\n",
        "        ReactApp[Vite/React Container] -- \"Fetches pre-calculated JSON\" --> API\n",
        "    end\n",
        "\n",
        "    User[User Browser] --> ReactApp\n",
        "```\n",
        "\n",
        "-----\n",
        "\n",
        "### Phase 1: Supabase Database Schema\n",
        "\n",
        "[cite\\_start]You need to set up your tables in Supabase to match the data structures defined in the Python code[cite: 34, 704]. Run these SQL queries in your Supabase SQL Editor.\n",
        "\n",
        "```sql\n",
        "-- Enable PostGIS if you want advanced map features later\n",
        "create extension if not exists postgis schema extensions;\n",
        "\n",
        "-- 1. STR Properties Table (The \"Registry Data\")\n",
        "[cite_start]-- Based on CANONICAL_COLUMNS [cite: 34]\n",
        "create table public.str_properties (\n",
        "    registration_id text primary key,\n",
        "    address text,\n",
        "    city text default 'Flagstaff',\n",
        "    state text default 'AZ',\n",
        "    zip text,\n",
        "    latitude float,\n",
        "    longitude float,\n",
        "    neighborhood text,\n",
        "    bedrooms int,\n",
        "    bathrooms float,\n",
        "    square_feet int,\n",
        "    year_built int,\n",
        "    license_status text,\n",
        "    occupancy_rate_estimate float,\n",
        "    adr_estimate float,\n",
        "    annual_revenue_estimate float,\n",
        "    review_score float,\n",
        "    list_price float,\n",
        "    days_on_market int,\n",
        "    listing_type text, -- 'MLS', 'FSBO'\n",
        "    estimated_value float,\n",
        "    equity_amount float,\n",
        "    equity_ratio float,\n",
        "    back_taxes_amount float,\n",
        "    has_recent_lawsuit boolean,\n",
        "    -- Atlas Calculated Segments & Scores\n",
        "    atlas_exploit_score float,\n",
        "    atlas_segment text, -- 'Distressed', 'Invest', 'Divest', 'Neutral'\n",
        "    updated_at timestamp with time zone default timezone('utc'::text, now())\n",
        ");\n",
        "\n",
        "-- 2. DID Re Leads Table (The \"Deal Scout\")\n",
        "[cite_start]-- Based on the SQLAlchemy Lead model [cite: 705]\n",
        "create table public.leads (\n",
        "    id bigint generated by default as identity primary key,\n",
        "    created_at timestamp with time zone default timezone('utc'::text, now()),\n",
        "    source text not null, -- e.g., 'reddit'\n",
        "    source_id text not null,\n",
        "    segment text not null, -- 'Distress', 'Invest', 'Divest'\n",
        "    agent_name text not null,\n",
        "    score float,\n",
        "    title text,\n",
        "    url text,\n",
        "    price float,\n",
        "    contact_handle text,\n",
        "    status text default 'New', -- New, Contacted, Dead\n",
        "    last_contacted_at timestamp with time zone,\n",
        "    times_contacted int default 0,\n",
        "    unique(source, source_id)\n",
        ");\n",
        "\n",
        "-- 3. Compliance Tables\n",
        "create table public.do_not_contact (\n",
        "    handle text primary key,\n",
        "    reason text,\n",
        "    created_at timestamp with time zone default timezone('utc'::text, now())\n",
        ");\n",
        "\n",
        "-- Add Row Level Security (RLS) so the public can't edit your data\n",
        "alter table public.str_properties enable row level security;\n",
        "alter table public.leads enable row level security;\n",
        "\n",
        "-- Policy: Everyone can read (for your frontend)\n",
        "create policy \"Public Read STR\" on public.str_properties for select using (true);\n",
        "create policy \"Public Read Leads\" on public.leads for select using (true);\n",
        "\n",
        "-- Policy: Only service_role (your Python worker) can insert/update\n",
        "-- (You configure this in Supabase Auth settings, not just SQL)\n",
        "```\n",
        "\n",
        "-----\n",
        "\n",
        "### Phase 2: The Python Worker (Backend)\n",
        "\n",
        "This script runs on Fly.io as a background process. It replaces the Streamlit script. It uses your existing logic but saves to Supabase instead of local files.\n",
        "\n",
        "**Requirements:** `pandas supabase firecrawl-py pytesseract praw beautifulsoup4 requests`\n",
        "\n",
        "#### `worker.py` (Conceptual Wrapper)\n",
        "\n",
        "This orchestrates the ingestion and scouting tools."
      ],
      "metadata": {
        "id": "91oLBRc17AZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from dotenv import load_dotenv\n",
        "from supabase import create_client, Client\n",
        "\n",
        "# Import your existing modules (slightly refactored for Supabase)\n",
        "# from atlas_ingestion import ingest_flagstaff_data\n",
        "# from deal_scout_didre import run_all_didre_agents\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# Initialize Supabase Client (requires service_role key for writing)\n",
        "SUPABASE_URL = os.getenv(\"SUPABASE_URL\")\n",
        "SUPABASE_SERVICE_KEY = os.getenv(\"SUPABASE_SERVICE_KEY\")\n",
        "supabase: Client = create_client(SUPABASE_URL, SUPABASE_SERVICE_KEY)\n",
        "\n",
        "def run_str_ingestion_job():\n",
        "    print(\"Starting STR Ingestion Job...\")\n",
        "    # [cite_start]1. Run the ingestion logic (SQL -> OCR -> Firecrawl) defined in the text [cite: 7-9].\n",
        "    # [cite_start]2. Run the scoring logic (Distressed/Invest/Divest)[cite: 133].\n",
        "    # 3. Get the resulting Pandas DataFrame.\n",
        "    # df = ingest_flagstaff_data()\n",
        "\n",
        "    # MOCK DATA FOR EXAMPLE: replace with actual df from your ingestion pipeline\n",
        "    df = pd.DataFrame([{\n",
        "        \"registration_id\": \"REG123\", \"address\": \"123 Main St\", \"city\": \"Flagstaff\",\n",
        "        \"bedrooms\": 3, \"atlas_segment\": \"Invest\", \"atlas_exploit_score\": 85.5\n",
        "    }])\n",
        "\n",
        "    # 4. Push to Supabase\n",
        "    print(f\"Upserting {len(df)} records to Supabase...\")\n",
        "    data = df.to_dict(orient='records')\n",
        "    try:\n",
        "        # Use upsert based on the primary key (registration_id)\n",
        "        response = supabase.table('str_properties').upsert(data).execute()\n",
        "        print(\"Supabase upsert successful.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error pushing to Supabase: {e}\")\n",
        "\n",
        "def run_didre_scout_job():\n",
        "    print(\"Starting DID Re Scout Job...\")\n",
        "    # [cite_start]This would call the logic from deal_scout_didre.py [cite: 613]\n",
        "    # The scouts would use the Supabase client to check DNC and insert leads.\n",
        "    # run_all_didre_agents(supabase_client=supabase)\n",
        "    print(\"Scout job finished.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # In a real Fly.io setup, you might use a scheduler like Celery,\n",
        "    # or just run this script via cron.\n",
        "    run_str_ingestion_job()\n",
        "    run_didre_scout_job()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "0WhNkXOH7AZX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Adapting the \"AI Examination Bot\" and \"DID Re Scouts\"\n",
        "\n",
        "You need to adapt the provided Python code to use the `supabase` client instead of SQLAlchemy sessions.\n",
        "\n",
        "[cite\\_start]**Example: Adapting DID Re Lead Insertion[cite: 709]:**"
      ],
      "metadata": {
        "id": "1RMCdb757AZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In atlas_leads_store.py adaptation\n",
        "def get_or_create_lead(supabase_client, *, agent_name, segment, source, source_id, title, url, score, **kwargs):\n",
        "    # Prepare data payload\n",
        "    data = {\n",
        "        \"agent_name\": agent_name, \"segment\": segment, \"source\": source,\n",
        "        \"source_id\": source_id, \"title\": title, \"url\": url, \"score\": score,\n",
        "        **kwargs\n",
        "    }\n",
        "    # Supabase upsert handles \"get or create\" based on the unique constraint\n",
        "    response = supabase_client.table('leads').upsert(data, on_conflict=\"source, source_id\").execute()\n",
        "    return response.data"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "g0vvaXRo7AZa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "### Phase 3: The Frontend (Vite + React)\n",
        "\n",
        "This replaces the Streamlit UI. It's \"super fast\" because it just fetches JSON from Supabase.\n",
        "\n",
        "**Setup:**\n",
        "\n",
        "```bash\n",
        "npm create vite@latest atlas-frontend -- --template react-ts\n",
        "cd atlas-frontend\n",
        "npm install @supabase/supabase-js @tanstack/react-query react-router-dom tailwindcss postcss autoprefixer\n",
        "# (Initialize TailwindCSS according to their docs)\n",
        "```\n",
        "\n",
        "#### 1\\. Supabase Client (`src/lib/supabase.ts`)\n",
        "\n",
        "```typescript\n",
        "import { createClient } from '@supabase/supabase-js';\n",
        "\n",
        "// Use the \"Anon\" public key here, not the service key.\n",
        "const supabaseUrl = import.meta.env.VITE_SUPABASE_URL;\n",
        "const supabaseAnonKey = import.meta.env.VITE_SUPABASE_ANON_KEY;\n",
        "\n",
        "export const supabase = createClient(supabaseUrl, supabaseAnonKey);\n",
        "```\n",
        "\n",
        "#### 2\\. Data Fetching Hook (`src/hooks/useSTRData.ts`)\n",
        "\n",
        "We use React Query to fetch data and cache it. This makes subsequent page loads instant.\n",
        "\n",
        "```typescript\n",
        "import { useQuery } from '@tanstack/react-query';\n",
        "import { supabase } from '../lib/supabase';\n",
        "\n",
        "export const useSTRData = () => {\n",
        "  return useQuery({\n",
        "    queryKey: ['str_properties'],\n",
        "    queryFn: async () => {\n",
        "      // \"Preload JSON\": This is just a fast REST call to Supabase\n",
        "      const { data, error } = await supabase\n",
        "        .from('str_properties')\n",
        "        .select('*')\n",
        "        .order('atlas_exploit_score', { ascending: false })\n",
        "        .limit(500); // Cap result size for speed\n",
        "\n",
        "      if (error) throw error;\n",
        "      return data;\n",
        "    },\n",
        "    staleTime: 1000 * 60 * 5, // Consider data fresh for 5 minutes\n",
        "  });\n",
        "};\n",
        "```\n",
        "\n",
        "#### 3\\. The Dashboard Page (`src/pages/Dashboard.tsx`)\n",
        "\n",
        "[cite\\_start]This replaces the main Streamlit overview and dataframes[cite: 161, 170].\n",
        "\n",
        "```tsx\n",
        "import React from 'react';\n",
        "import { useSTRData } from '../hooks/useSTRData';\n",
        "\n",
        "const Dashboard = () => {\n",
        "  // This hook handles loading states and caching automatically\n",
        "  const { data: properties, isLoading, error } = useSTRData();\n",
        "\n",
        "  if (isLoading) return <div className=\"p-8\">Loading Atlas brain...</div>;\n",
        "  if (error) return <div className=\"p-8 text-red-500\">Error loading data.</div>;\n",
        "\n",
        "  // Simple metric calculations on the client side\n",
        "  const total = properties?.length || 0;\n",
        "  const distressed = properties?.filter(p => p.atlas_segment === 'Distressed').length;\n",
        "\n",
        "  return (\n",
        "    <div className=\"p-6 bg-gray-100 min-h-screen\">\n",
        "      <h1 className=\"text-3xl font-bold mb-6\">Atlas: Command Center</h1>\n",
        "\n",
        "      [cite_start]{/* Metrics Section [cite: 165] */}\n",
        "      <div className=\"grid grid-cols-1 md:grid-cols-4 gap-4 mb-8\">\n",
        "        <MetricCard title=\"Total STRs Tracks\" value={total} />\n",
        "        <MetricCard title=\"Distressed Targets\" value={distressed} color=\"red\" />\n",
        "        {/* Add Invest/Divest metrics here */}\n",
        "      </div>\n",
        "\n",
        "      [cite_start]{/* Data Table Section [cite: 170] */}\n",
        "      <div className=\"bg-white shadow rounded-lg overflow-hidden\">\n",
        "        <table className=\"min-w-full divide-y divide-gray-200\">\n",
        "          <thead className=\"bg-gray-50\">\n",
        "            <tr>\n",
        "              <th className=\"px-6 py-3 text-left text-xs font-medium text-gray-500 uppercase tracking-wider\">Score</th>\n",
        "              <th className=\"px-6 py-3 text-left text-xs font-medium text-gray-500 uppercase tracking-wider\">Segment</th>\n",
        "              <th className=\"px-6 py-3 text-left text-xs font-medium text-gray-500 uppercase tracking-wider\">Address</th>\n",
        "              <th className=\"px-6 py-3 text-left text-xs font-medium text-gray-500 uppercase tracking-wider\">Beds</th>\n",
        "            </tr>\n",
        "          </thead>\n",
        "          <tbody className=\"bg-white divide-y divide-gray-200\">\n",
        "            {properties?.slice(0, 50).map((prop) => (\n",
        "              <tr key={prop.registration_id} className=\"hover:bg-gray-50\">\n",
        "                <td className=\"px-6 py-4 whitespace-nowrap font-bold\">{prop.atlas_exploit_score?.toFixed(0)}</td>\n",
        "                <td className=\"px-6 py-4 whitespace-nowrap\">\n",
        "                    <span className={`px-2 inline-flex text-xs leading-5 font-semibold rounded-full\n",
        "                        ${prop.atlas_segment === 'Distressed' ? 'bg-red-100 text-red-800' :\n",
        "                        prop.atlas_segment === 'Invest' ? 'bg-green-100 text-green-800' : 'bg-gray-100 text-gray-800'}`}>\n",
        "                    {prop.atlas_segment}\n",
        "                    </span>\n",
        "                </td>\n",
        "                <td className=\"px-6 py-4 whitespace-nowrap\">{prop.address}</td>\n",
        "                <td className=\"px-6 py-4 whitespace-nowrap\">{prop.bedrooms}</td>\n",
        "              </tr>\n",
        "            ))}\n",
        "          </tbody>\n",
        "        </table>\n",
        "      </div>\n",
        "    </div>\n",
        "  );\n",
        "};\n",
        "\n",
        "// Simple UI helper\n",
        "const MetricCard = ({ title, value, color = \"blue\" }: any) => (\n",
        "    <div className={`bg-white p-4 rounded-lg shadow border-l-4 border-${color}-500`}>\n",
        "        <h3 className=\"text-gray-500 text-sm uppercase\">{title}</h3>\n",
        "        <p className=\"text-2xl font-bold\">{value}</p>\n",
        "    </div>\n",
        ");\n",
        "\n",
        "export default Dashboard;\n",
        "```\n",
        "\n",
        "### Deployment on Fly.io\n",
        "\n",
        "You will have two separate Fly apps.\n",
        "\n",
        "**1. The Frontend App (`fly.toml` for Vite app)**\n",
        "This is a simple static site host.\n",
        "\n",
        "```toml\n",
        "app = \"atlas-frontend\"\n",
        "primary_region = \"lax\"\n",
        "\n",
        "[build]\n",
        "  # Use a standard Node/Nginx builder for static sites\n",
        "  builder = \"paketobuildpacks/builder:base\"\n",
        "  buildpacks = [\"gcr.io/paketo-buildpacks/nodejs\", \"gcr.io/paketo-buildpacks/nginx\"]\n",
        "\n",
        "[[services]]\n",
        "  internal_port = 8080\n",
        "  protocol = \"tcp\"\n",
        "  [services.concurrency]\n",
        "    hard_limit = 25\n",
        "    soft_limit = 20\n",
        "```\n",
        "\n",
        "**2. The Backend Worker App (`fly.toml` for Python)**\n",
        "This runs your Python script continuously or via cron. It doesn't need open ports if it's just a worker.\n",
        "\n",
        "```toml\n",
        "app = \"atlas-worker\"\n",
        "primary_region = \"lax\"\n",
        "\n",
        "[build]\n",
        "  dockerfile = \"Dockerfile.worker\" # You'll need to write this\n",
        "\n",
        "[env]\n",
        "  # Set your Supabase Service Key and DB URL here as secrets\n",
        "  # fly secrets set SUPABASE_SERVICE_KEY=...\n",
        "```"
      ],
      "metadata": {
        "id": "0PwjnpvE7AZa"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}